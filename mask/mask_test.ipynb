{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from functorch.dim import dims\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torchvision.ops import roi_align\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roi_Align 부분, 서현님 part\n",
    "def bilinear_interpolate(input, height, width, y, x, ymask, xmask):\n",
    "    \n",
    "    y = y.clamp(min=0)\n",
    "    x = x.clamp(min=0)\n",
    "    y_low = y.int()\n",
    "    x_low = x.int()\n",
    "    y_high = torch.where(y_low >= height - 1, height - 1, y_low + 1)\n",
    "    y_low = torch.where(y_low >= height - 1, height - 1, y_low)\n",
    "    y = torch.where(y_low >= height - 1, y.to(input.dtype), y)\n",
    "  \n",
    "    x_high = torch.where(x_low >= width - 1, width - 1, x_low + 1)\n",
    "    x_low = torch.where(x_low >= width - 1, width - 1, x_low)\n",
    "    x = torch.where(x_low >= width - 1, x.to(input.dtype), x)\n",
    "    \n",
    "    ly = y - y_low\n",
    "    lx = x - x_low\n",
    "    hy = 1. - ly\n",
    "    hx = 1. - lx\n",
    "    \n",
    "    def masked_index(y, x):\n",
    "        y = torch.where(ymask, y, 0)\n",
    "        x = torch.where(xmask, x, 0)\n",
    "        return input[y, x]\n",
    "\n",
    "    v1 = masked_index(y_low, x_low)\n",
    "    v2 = masked_index(y_low, x_high)\n",
    "    v3 = masked_index(y_high, x_low)\n",
    "    v4 = masked_index(y_high, x_high)\n",
    "    w1 = hy * hx\n",
    "    w2 = hy * lx\n",
    "    w3 = ly * hx\n",
    "    w4 = ly * lx\n",
    "\n",
    "    val = w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4\n",
    "    return val\n",
    "\n",
    "def roi_align(input, rois, spatial_scale, pooled_height, pooled_width, sampling_ratio, aligned):\n",
    "    _, _, height, width = input.size()\n",
    "\n",
    "    n, c, ph, pw = dims(4)\n",
    "\n",
    "    ph.size = pooled_height\n",
    "    pw.size = pooled_width\n",
    "    offset_rois = rois[n]\n",
    "    roi_batch_ind = offset_rois[0].int()\n",
    "    offset = 0.5 if aligned else 0.0\n",
    "    roi_start_w = offset_rois[1] * spatial_scale - offset\n",
    "    roi_start_h = offset_rois[2] * spatial_scale - offset\n",
    "    roi_end_w = offset_rois[3] * spatial_scale - offset\n",
    "    roi_end_h = offset_rois[4] * spatial_scale - offset\n",
    "\n",
    "    roi_width = roi_end_w - roi_start_w\n",
    "    roi_height = roi_end_h - roi_start_h\n",
    "    if not aligned:\n",
    "        roi_width = torch.clamp(roi_width, min=1.0)\n",
    "        roi_height = torch.clamp(roi_height, min=1.0)\n",
    "\n",
    "    bin_size_h = roi_height / pooled_height\n",
    "    bin_size_w = roi_width / pooled_width\n",
    "\n",
    "    offset_input = input[roi_batch_ind][c]\n",
    "\n",
    "    roi_bin_grid_h = sampling_ratio if sampling_ratio > 0 else torch.ceil(roi_height / pooled_height)\n",
    "    roi_bin_grid_w = sampling_ratio if sampling_ratio > 0 else torch.ceil(roi_width / pooled_width)\n",
    "\n",
    "    count = torch.clamp(roi_bin_grid_h * roi_bin_grid_w, min=1)\n",
    "\n",
    "    iy, ix = dims(2)\n",
    "\n",
    "    iy.size = height  # < roi_bin_grid_h\n",
    "    ix.size = width  # < roi_bin_grid_w\n",
    "    \n",
    "    y = roi_start_h + ph * bin_size_h + (iy + 0.5) * bin_size_h / roi_bin_grid_h\n",
    "    x = roi_start_w + pw * bin_size_w + (ix + 0.5) * bin_size_w / roi_bin_grid_w\n",
    "    ymask = iy < roi_bin_grid_h\n",
    "    xmask = ix < roi_bin_grid_w\n",
    "    val = bilinear_interpolate(offset_input, height, width, y, x, ymask, xmask)\n",
    "    val = torch.where(ymask, val, 0)\n",
    "    val = torch.where(xmask, val, 0)\n",
    "    output = val.sum((iy, ix))\n",
    "    output /= count\n",
    "\n",
    "    return output.order(n, c, ph, pw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# featuremap을 얻기 위한 컨볼루션 부분\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.avg_pooling = nn.AdaptiveAvgPool2d((28,28))\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.avg_pooling(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "simple_cnn = CNN()\n",
    "\n",
    "# coco 이미지 불러오기\n",
    "image_path = \"COCO_train2014_000000000030.jpg\"\n",
    "image = Image.open(image_path)\n",
    "\n",
    "#텐서 변환\n",
    "preprocess = transforms.Compose([transforms.ToTensor(),])\n",
    "\n",
    "input_image = preprocess(image)\n",
    "input_image = input_image.unsqueeze(0)\n",
    "\n",
    "output_image = simple_cnn(input_image)\n",
    "\n",
    "\n",
    "# 임의의 피쳐맵 생성\n",
    "# featuremap은 cnn을 통과한 이미지의 output을 그대로 가져옴.\n",
    "# rois는 임의로 설정.\n",
    "features = output_image\n",
    "print(features.shape)\n",
    "rois = torch.tensor([\n",
    "    [0, 5, 5, 10, 10],\n",
    "    [0, 15, 15, 20, 20],\n",
    "    [0, 2, 3, 5, 5]\n",
    "], dtype=torch.float)\n",
    "\n",
    "\n",
    "#roi를 통과한 output size 설정,\n",
    "#spatial_scale 은 28,28 사이즈를 7,7로 바꿔야하므로 4.0으로 변경\n",
    "output_size = (7, 7)\n",
    "spatial_scale = 1.0 / 4.0\n",
    "\n",
    "# Call the roi_align function\n",
    "pooled_features = roi_align(features, rois, spatial_scale, output_size[0], output_size[1], -1, False)\n",
    "\n",
    "print(pooled_features.sum())\n",
    "\n",
    "from torchvision.ops import roi_align as roi_align_torchvision\n",
    "\n",
    "print(roi_align_torchvision(features, rois, output_size, spatial_scale).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#마스크부분 & sampadding은 메소드인 padding ='same'과 동일하다.\n",
    "\n",
    "class SamePad2d(nn.Module):\n",
    "    \"\"\"Mimics tensorflow's 'SAME' padding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super(SamePad2d, self).__init__()\n",
    "        self.kernel_size = torch.nn.modules.utils._pair(kernel_size)\n",
    "        self.stride = torch.nn.modules.utils._pair(stride)\n",
    "\n",
    "    def forward(self, input):\n",
    "        in_width = input.size()[2]\n",
    "        in_height = input.size()[3]\n",
    "        out_width = math.ceil(float(in_width) / float(self.stride[0]))\n",
    "        out_height = math.ceil(float(in_height) / float(self.stride[1]))\n",
    "        pad_along_width = ((out_width - 1) * self.stride[0] +\n",
    "                           self.kernel_size[0] - in_width)\n",
    "        pad_along_height = ((out_height - 1) * self.stride[1] +\n",
    "                            self.kernel_size[1] - in_height)\n",
    "        pad_left = math.floor(pad_along_width / 2)\n",
    "        pad_top = math.floor(pad_along_height / 2)\n",
    "        pad_right = pad_along_width - pad_left\n",
    "        pad_bottom = pad_along_height - pad_top\n",
    "        return F.pad(input, (pad_left, pad_right, pad_top, pad_bottom), 'constant', 0)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__\n",
    "  \n",
    "  \n",
    "#단일 피쳐맵의 실험을 할때는 batch_size가 필요하지 않아서, 주석처리\n",
    "# self.batch_size = batch_size 이부분을 제외함.\n",
    "# 현재 단일 이미지의 경우 [3,512,7,7]의 shape을 가진 tensor로 출력됨.\n",
    "# num_classes값은 기학습 모델 기준으로 80개의 class 개수를 가지고 있어서\n",
    "# 80으로 넣어줄 것.\n",
    "# 현재 기준 roi_align의 output이 [num_rois,in_channels,pool_height,pool_weight]이므로\n",
    "# 생성자에서 따로 생성할 필요가 없음\n",
    "# 단, 첫 self.conv1의 입력값은 512로 맞춰줘야함. roi_aling에서 512채널이 나왔기 때문\n",
    "\n",
    "class Mask(nn.Module):\n",
    "  def __init__(self, num_classes):\n",
    "      super(Mask, self).__init__()\n",
    "      self.num_classes = num_classes\n",
    "      self.padding = SamePad2d(kernel_size=3,stride=1)\n",
    "      self.conv1 = nn.Conv2d(512, 256, kernel_size=3, stride=1)\n",
    "      self.bn1 = nn.BatchNorm2d(256, eps=0.001)\n",
    "      self.deconv = nn.ConvTranspose2d(256, 80, kernel_size=2, stride=2)\n",
    "      self.conv2 = nn.Conv2d(80,self.num_classes, kernel_size=3, stride=1)\n",
    "      self.sigmoid = nn.Sigmoid()\n",
    "      self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv1(self.padding(x))\n",
    "    x = self.bn1(x)\n",
    "    x = self.relu(x)\n",
    "    x = self.deconv(x)\n",
    "    x = self.conv2(self.padding(x))\n",
    "    x = self.sigmoid(x)\n",
    "    p_mask = x\n",
    "    return p_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mask 클래스를 객체로 받아주며, num_classes값을 만들어줘야함.\n",
    "mask = Mask(num_classes=80)\n",
    "mask_out = mask(pooled_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 마스크 loss \n",
    "mask_prediction = mask_out  # 모델이 예측한 마스크 값\n",
    "mask_target = torch.rand_like(mask_out, dtype=torch.float)  # 랜덤한 실제 마스크 값, 실제 데이터에 따라 적절한 값을 사용해야 합니다.\n",
    "\n",
    "# BCELoss를 사용하여 마스크 손실 계산\n",
    "mask_criterion = nn.BCELoss()\n",
    "mask_loss = mask_criterion(mask_prediction, mask_target)\n",
    "\n",
    "# 마스크 손실 출력\n",
    "print(\"Mask Loss:\", mask_loss.item())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
